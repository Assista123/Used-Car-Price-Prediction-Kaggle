{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bba60f9f",
   "metadata": {
    "papermill": {
     "duration": 0.003863,
     "end_time": "2025-09-12T18:08:15.483392",
     "exception": false,
     "start_time": "2025-09-12T18:08:15.479529",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Install AutoGluon\n",
    "\n",
    "Installs the latest version of AutoGluon, a library for automated machine learning on tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16562840",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:08:15.491759Z",
     "iopub.status.busy": "2025-09-12T18:08:15.491446Z",
     "iopub.status.idle": "2025-09-12T18:08:29.621514Z",
     "shell.execute_reply": "2025-09-12T18:08:29.620492Z"
    },
    "papermill": {
     "duration": 14.136381,
     "end_time": "2025-09-12T18:08:29.623183",
     "exception": false,
     "start_time": "2025-09-12T18:08:15.486802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autogluon.tabular\r\n",
      "  Downloading autogluon.tabular-1.4.0-py3-none-any.whl.metadata (16 kB)\r\n",
      "Requirement already satisfied: numpy<2.4.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular) (1.26.4)\r\n",
      "Requirement already satisfied: scipy<1.17,>=1.5.4 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular) (1.15.3)\r\n",
      "Requirement already satisfied: pandas<2.4.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular) (2.2.3)\r\n",
      "Collecting scikit-learn<1.8.0,>=1.4.0 (from autogluon.tabular)\r\n",
      "  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: networkx<4,>=3.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular) (3.5)\r\n",
      "Collecting autogluon.core==1.4.0 (from autogluon.tabular)\r\n",
      "  Downloading autogluon.core-1.4.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting autogluon.features==1.4.0 (from autogluon.tabular)\r\n",
      "  Downloading autogluon.features-1.4.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: tqdm<5,>=4.38 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.4.0->autogluon.tabular) (4.67.1)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.4.0->autogluon.tabular) (2.32.4)\r\n",
      "Requirement already satisfied: matplotlib<3.11,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.4.0->autogluon.tabular) (3.7.2)\r\n",
      "Requirement already satisfied: boto3<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.4.0->autogluon.tabular) (1.39.1)\r\n",
      "Collecting autogluon.common==1.4.0 (from autogluon.core==1.4.0->autogluon.tabular)\r\n",
      "  Downloading autogluon.common-1.4.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: pyarrow<21.0.0,>=7.0.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.common==1.4.0->autogluon.core==1.4.0->autogluon.tabular) (19.0.1)\r\n",
      "Requirement already satisfied: psutil<7.1.0,>=5.7.3 in /usr/local/lib/python3.11/dist-packages (from autogluon.common==1.4.0->autogluon.core==1.4.0->autogluon.tabular) (7.0.0)\r\n",
      "Requirement already satisfied: joblib<1.7,>=1.2 in /usr/local/lib/python3.11/dist-packages (from autogluon.common==1.4.0->autogluon.core==1.4.0->autogluon.tabular) (1.5.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.4.0,>=1.25.0->autogluon.tabular) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.4.0,>=1.25.0->autogluon.tabular) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.4.0,>=1.25.0->autogluon.tabular) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.4.0,>=1.25.0->autogluon.tabular) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.4.0,>=1.25.0->autogluon.tabular) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.4.0,>=1.25.0->autogluon.tabular) (2.4.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.4.0,>=2.0.0->autogluon.tabular) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.4.0,>=2.0.0->autogluon.tabular) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.4.0,>=2.0.0->autogluon.tabular) (2025.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.8.0,>=1.4.0->autogluon.tabular) (3.6.0)\r\n",
      "Requirement already satisfied: botocore<1.40.0,>=1.39.1 in /usr/local/lib/python3.11/dist-packages (from boto3<2,>=1.10->autogluon.core==1.4.0->autogluon.tabular) (1.39.1)\r\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3<2,>=1.10->autogluon.core==1.4.0->autogluon.tabular) (1.0.1)\r\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3<2,>=1.10->autogluon.core==1.4.0->autogluon.tabular) (0.13.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.tabular) (1.3.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.tabular) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.tabular) (4.58.4)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.tabular) (1.4.8)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.tabular) (25.0)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.tabular) (11.2.1)\r\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.tabular) (3.0.9)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<2.4.0,>=2.0.0->autogluon.tabular) (1.17.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.4.0,>=1.25.0->autogluon.tabular) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.4.0,>=1.25.0->autogluon.tabular) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.4.0,>=1.25.0->autogluon.tabular) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.4.0,>=1.25.0->autogluon.tabular) (2024.2.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.4.0->autogluon.tabular) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.4.0->autogluon.tabular) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.4.0->autogluon.tabular) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.4.0->autogluon.tabular) (2025.6.15)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.4.0,>=1.25.0->autogluon.tabular) (2024.2.0)\r\n",
      "Downloading autogluon.tabular-1.4.0-py3-none-any.whl (487 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.3/487.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading autogluon.core-1.4.0-py3-none-any.whl (225 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading autogluon.features-1.4.0-py3-none-any.whl (64 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading autogluon.common-1.4.0-py3-none-any.whl (70 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: scikit-learn, autogluon.common, autogluon.features, autogluon.core, autogluon.tabular\r\n",
      "  Attempting uninstall: scikit-learn\r\n",
      "    Found existing installation: scikit-learn 1.2.2\r\n",
      "    Uninstalling scikit-learn-1.2.2:\r\n",
      "      Successfully uninstalled scikit-learn-1.2.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed autogluon.common-1.4.0 autogluon.core-1.4.0 autogluon.features-1.4.0 autogluon.tabular-1.4.0 scikit-learn-1.7.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U autogluon.tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b3528",
   "metadata": {
    "papermill": {
     "duration": 0.004416,
     "end_time": "2025-09-12T18:08:29.633017",
     "exception": false,
     "start_time": "2025-09-12T18:08:29.628601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import Libraries\n",
    "\n",
    "Imports essential libraries for data manipulation, visualization, and modeling, including AutoGluon, LightGBM, CatBoost, XGBoost, and scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c5b068e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-12T18:08:29.644018Z",
     "iopub.status.busy": "2025-09-12T18:08:29.643672Z",
     "iopub.status.idle": "2025-09-12T18:08:40.509188Z",
     "shell.execute_reply": "2025-09-12T18:08:40.508292Z"
    },
    "papermill": {
     "duration": 10.873335,
     "end_time": "2025-09-12T18:08:40.511036",
     "exception": false,
     "start_time": "2025-09-12T18:08:29.637701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from lightgbm import log_evaluation, early_stopping\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab4d5c6",
   "metadata": {
    "papermill": {
     "duration": 0.004588,
     "end_time": "2025-09-12T18:08:40.520595",
     "exception": false,
     "start_time": "2025-09-12T18:08:40.516007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import Datasets\n",
    "\n",
    "Loads the training and test datasets for the competition, along with the original used car dataset. Preprocesses the 'milage' and 'price' columns in the original dataset to extract numeric values. Removes the 'id' column from both train and test datasets, and concatenates the original dataset to the training data to increase sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a49baba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:08:40.531556Z",
     "iopub.status.busy": "2025-09-12T18:08:40.530848Z",
     "iopub.status.idle": "2025-09-12T18:08:41.908166Z",
     "shell.execute_reply": "2025-09-12T18:08:41.907418Z"
    },
    "papermill": {
     "duration": 1.384459,
     "end_time": "2025-09-12T18:08:41.909768",
     "exception": false,
     "start_time": "2025-09-12T18:08:40.525309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/hackathon-qualification/archive/train.csv\")\n",
    "test  = pd.read_csv(\"/kaggle/input/hackathon-qualification/archive/test.csv\")\n",
    "Original = pd.read_csv('/kaggle/input/used-car-price-prediction-dataset/used_cars.csv')\n",
    "Original[['milage', 'price']] = Original[['milage', 'price']].map(\n",
    "    lambda x: int(''.join(re.findall(r'\\d+', x))))\n",
    "\n",
    "\n",
    "train.drop(columns=['id'], inplace=True)\n",
    "test.drop(columns=['id'], inplace=True)\n",
    "\n",
    "\n",
    "train = pd.concat([train, Original], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87953545",
   "metadata": {
    "papermill": {
     "duration": 0.00467,
     "end_time": "2025-09-12T18:08:41.919294",
     "exception": false,
     "start_time": "2025-09-12T18:08:41.914624",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Defines functions to create new features:\n",
    "- `extract_age_features` calculates vehicle age and mileage per year.\n",
    "- `extract_other_features` flags luxury brands.\n",
    "Applies these feature engineering steps to both train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "396e66ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:08:41.929962Z",
     "iopub.status.busy": "2025-09-12T18:08:41.929620Z",
     "iopub.status.idle": "2025-09-12T18:08:41.982440Z",
     "shell.execute_reply": "2025-09-12T18:08:41.981459Z"
    },
    "papermill": {
     "duration": 0.060309,
     "end_time": "2025-09-12T18:08:41.984142",
     "exception": false,
     "start_time": "2025-09-12T18:08:41.923833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_age_features(df):\n",
    "    current_year = 2024\n",
    "\n",
    "    df['Vehicle_Age'] = current_year - df['model_year']\n",
    "    \n",
    "    df['Mileage_per_Year'] = df['milage'] / df['Vehicle_Age']\n",
    "    df['milage_with_age'] =  df.groupby('Vehicle_Age')['milage'].transform('mean')\n",
    "    \n",
    "    df['Mileage_per_Year_with_age'] =  df.groupby('Vehicle_Age')['Mileage_per_Year'].transform('mean')\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = extract_age_features(train)\n",
    "test = extract_age_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d26bd8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:08:41.995537Z",
     "iopub.status.busy": "2025-09-12T18:08:41.995181Z",
     "iopub.status.idle": "2025-09-12T18:08:42.154055Z",
     "shell.execute_reply": "2025-09-12T18:08:42.153083Z"
    },
    "papermill": {
     "duration": 0.165997,
     "end_time": "2025-09-12T18:08:42.155787",
     "exception": false,
     "start_time": "2025-09-12T18:08:41.989790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_other_features(df):\n",
    "    \n",
    "    luxury_brands =  ['Mercedes-Benz', 'BMW', 'Audi', 'Porsche', 'Land', \n",
    "                    'Lexus', 'Jaguar', 'Bentley', 'Maserati', 'Lamborghini', \n",
    "                    'Rolls-Royce', 'Ferrari', 'McLaren', 'Aston', 'Maybach']\n",
    "    df['Is_Luxury_Brand'] = df['brand'].apply(lambda x: 1 if x in luxury_brands else 0)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    return df\n",
    "train = extract_other_features(train)\n",
    "test = extract_other_features(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83ca7ba",
   "metadata": {
    "papermill": {
     "duration": 0.004788,
     "end_time": "2025-09-12T18:08:42.165395",
     "exception": false,
     "start_time": "2025-09-12T18:08:42.160607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Categorical Feature Processing\n",
    "\n",
    "Handles rare categories by replacing them with 'noise', fills missing values, and converts categorical columns to the 'category' dtype for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9182c19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:08:42.175752Z",
     "iopub.status.busy": "2025-09-12T18:08:42.175441Z",
     "iopub.status.idle": "2025-09-12T18:08:42.841538Z",
     "shell.execute_reply": "2025-09-12T18:08:42.840725Z"
    },
    "papermill": {
     "duration": 0.673236,
     "end_time": "2025-09-12T18:08:42.843140",
     "exception": false,
     "start_time": "2025-09-12T18:08:42.169904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update(df):\n",
    "    \n",
    "    t = 100\n",
    "    \n",
    "    cat_c = ['brand','model','fuel_type','engine','transmission','ext_col','int_col','accident','clean_title',\n",
    "             \n",
    "            ]\n",
    "    re_ = ['model','engine','transmission','ext_col','int_col']\n",
    "    \n",
    "    for col in re_:\n",
    "        df.loc[df[col].value_counts(dropna=False)[df[col]].values < t, col] = \"noise\"\n",
    "        \n",
    "    for col in cat_c:\n",
    "        df[col] = df[col].fillna('missing')\n",
    "        df[col] = df[col].astype('category')\n",
    "        \n",
    "    return df\n",
    "\n",
    "train  = update(train)\n",
    "test   = update(test)\n",
    "\n",
    "X = train.drop('price', axis=1)\n",
    "y = train['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9545d04",
   "metadata": {
    "papermill": {
     "duration": 0.00438,
     "end_time": "2025-09-12T18:08:42.852242",
     "exception": false,
     "start_time": "2025-09-12T18:08:42.847862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Training (LightGBM & CatBoost)\n",
    "\n",
    "Trains LightGBM or CatBoost models using K-Fold cross-validation, returning out-of-fold predictions and trained models. Uses both MAE and MSE objectives, and stores their predictions for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76c9aa37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:08:42.863308Z",
     "iopub.status.busy": "2025-09-12T18:08:42.862507Z",
     "iopub.status.idle": "2025-09-12T18:09:42.923348Z",
     "shell.execute_reply": "2025-09-12T18:09:42.922541Z"
    },
    "papermill": {
     "duration": 60.068213,
     "end_time": "2025-09-12T18:09:42.925061",
     "exception": false,
     "start_time": "2025-09-12T18:08:42.856848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_cols--------['brand', 'model', 'fuel_type', 'engine', 'transmission', 'ext_col', 'int_col', 'accident', 'clean_title']\n",
      "Training fold 1/5 with LGBM\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1724\n",
      "[LightGBM] [Info] Number of data points in the train set: 154033, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 30825.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[300]\ttrain's l1: 16379.4\tvalid's l1: 16910.1\n",
      "Early stopping, best iteration is:\n",
      "[369]\ttrain's l1: 16314.7\tvalid's l1: 16906.2\n",
      "LGBM Fold RMSE: 67861.26982972873\n",
      "Training fold 2/5 with LGBM\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006859 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1724\n",
      "[LightGBM] [Info] Number of data points in the train set: 154033, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 30900.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[300]\ttrain's l1: 16173.3\tvalid's l1: 17720.9\n",
      "Early stopping, best iteration is:\n",
      "[296]\ttrain's l1: 16177.2\tvalid's l1: 17720.1\n",
      "LGBM Fold RMSE: 84983.07894324511\n",
      "Training fold 3/5 with LGBM\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1723\n",
      "[LightGBM] [Info] Number of data points in the train set: 154034, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 30798.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[300]\ttrain's l1: 16235.3\tvalid's l1: 17551.8\n",
      "[600]\ttrain's l1: 16004.4\tvalid's l1: 17543.7\n",
      "[900]\ttrain's l1: 15814.8\tvalid's l1: 17536.3\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[915]\ttrain's l1: 15808.4\tvalid's l1: 17535.6\n",
      "LGBM Fold RMSE: 76582.94024428958\n",
      "Training fold 4/5 with LGBM\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1724\n",
      "[LightGBM] [Info] Number of data points in the train set: 154034, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 30900.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[300]\ttrain's l1: 16445.6\tvalid's l1: 16624.6\n",
      "Early stopping, best iteration is:\n",
      "[321]\ttrain's l1: 16427.7\tvalid's l1: 16623.4\n",
      "LGBM Fold RMSE: 66922.49091908698\n",
      "Training fold 5/5 with LGBM\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1723\n",
      "[LightGBM] [Info] Number of data points in the train set: 154034, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 30798.000000\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[300]\ttrain's l1: 16408.8\tvalid's l1: 16841.1\n",
      "[600]\ttrain's l1: 16178.4\tvalid's l1: 16837.3\n",
      "Early stopping, best iteration is:\n",
      "[632]\ttrain's l1: 16161.2\tvalid's l1: 16835.6\n",
      "LGBM Fold RMSE: 69984.17808875497\n",
      "Mean RMSE: 73266.79160502108\n",
      "Training fold 1/5 with LGBM\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006876 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1979\n",
      "[LightGBM] [Info] Number of data points in the train set: 154033, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 43958.172307\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[22]\ttrain's l2: 5.0106e+09\tvalid's l2: 4.5401e+09\n",
      "LGBM Fold RMSE: 67380.26176624796\n",
      "Training fold 2/5 with LGBM\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1979\n",
      "[LightGBM] [Info] Number of data points in the train set: 154033, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 43756.952828\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\ttrain's l2: 4.39034e+09\tvalid's l2: 7.08955e+09\n",
      "LGBM Fold RMSE: 84199.46679070957\n",
      "Training fold 3/5 with LGBM\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007590 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1978\n",
      "[LightGBM] [Info] Number of data points in the train set: 154034, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 43759.840003\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[33]\ttrain's l2: 4.5551e+09\tvalid's l2: 5.72881e+09\n",
      "LGBM Fold RMSE: 75688.92399392725\n",
      "Training fold 4/5 with LGBM\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007187 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1979\n",
      "[LightGBM] [Info] Number of data points in the train set: 154034, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 44054.894439\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttrain's l2: 5.08327e+09\tvalid's l2: 4.4184e+09\n",
      "LGBM Fold RMSE: 66471.01636437845\n",
      "Training fold 5/5 with LGBM\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007593 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1978\n",
      "[LightGBM] [Info] Number of data points in the train set: 154034, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 43930.511322\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\ttrain's l2: 4.98447e+09\tvalid's l2: 4.79976e+09\n",
      "LGBM Fold RMSE: 69280.32513245776\n",
      "Mean RMSE: 72603.99880954421\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>model_year</th>\n",
       "      <th>milage</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>engine</th>\n",
       "      <th>transmission</th>\n",
       "      <th>ext_col</th>\n",
       "      <th>int_col</th>\n",
       "      <th>accident</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>Vehicle_Age</th>\n",
       "      <th>Mileage_per_Year</th>\n",
       "      <th>milage_with_age</th>\n",
       "      <th>Mileage_per_Year_with_age</th>\n",
       "      <th>Is_Luxury_Brand</th>\n",
       "      <th>LGBM_MAE</th>\n",
       "      <th>LGBM_MSE_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Land</td>\n",
       "      <td>noise</td>\n",
       "      <td>2015</td>\n",
       "      <td>98000</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>240.0HP 2.0L 4 Cylinder Engine Gasoline Fuel</td>\n",
       "      <td>6-Speed A/T</td>\n",
       "      <td>White</td>\n",
       "      <td>Beige</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "      <td>9</td>\n",
       "      <td>10888.888889</td>\n",
       "      <td>81078.503981</td>\n",
       "      <td>9008.722665</td>\n",
       "      <td>1</td>\n",
       "      <td>16402.458527</td>\n",
       "      <td>4280.396289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Land</td>\n",
       "      <td>Rover Defender SE</td>\n",
       "      <td>2020</td>\n",
       "      <td>9142</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>395.0HP 3.0L Straight 6 Cylinder Engine Gasoli...</td>\n",
       "      <td>8-Speed A/T</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Black</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4</td>\n",
       "      <td>2285.500000</td>\n",
       "      <td>34258.886442</td>\n",
       "      <td>8564.721611</td>\n",
       "      <td>1</td>\n",
       "      <td>57252.126782</td>\n",
       "      <td>15875.930352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ford</td>\n",
       "      <td>Expedition Limited</td>\n",
       "      <td>2022</td>\n",
       "      <td>28121</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>3.5L V6 24V PDI DOHC Twin Turbo</td>\n",
       "      <td>10-Speed Automatic</td>\n",
       "      <td>White</td>\n",
       "      <td>Ebony</td>\n",
       "      <td>None reported</td>\n",
       "      <td>missing</td>\n",
       "      <td>2</td>\n",
       "      <td>14060.500000</td>\n",
       "      <td>17877.043403</td>\n",
       "      <td>8938.521702</td>\n",
       "      <td>0</td>\n",
       "      <td>49451.472824</td>\n",
       "      <td>8181.763131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Audi</td>\n",
       "      <td>noise</td>\n",
       "      <td>2016</td>\n",
       "      <td>61258</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>2.0 Liter TFSI</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>noise</td>\n",
       "      <td>Black</td>\n",
       "      <td>None reported</td>\n",
       "      <td>missing</td>\n",
       "      <td>8</td>\n",
       "      <td>7657.250000</td>\n",
       "      <td>75999.679762</td>\n",
       "      <td>9499.959970</td>\n",
       "      <td>1</td>\n",
       "      <td>25768.824542</td>\n",
       "      <td>3535.516228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Audi</td>\n",
       "      <td>A6 2.0T Premium Plus</td>\n",
       "      <td>2018</td>\n",
       "      <td>59000</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>252.0HP 2.0L 4 Cylinder Engine Gasoline Fuel</td>\n",
       "      <td>A/T</td>\n",
       "      <td>Gray</td>\n",
       "      <td>Black</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "      <td>6</td>\n",
       "      <td>9833.333333</td>\n",
       "      <td>52105.532436</td>\n",
       "      <td>8684.255406</td>\n",
       "      <td>1</td>\n",
       "      <td>27677.661661</td>\n",
       "      <td>4005.789445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  brand                 model  model_year  milage fuel_type  \\\n",
       "0  Land                 noise        2015   98000  Gasoline   \n",
       "1  Land     Rover Defender SE        2020    9142    Hybrid   \n",
       "2  Ford    Expedition Limited        2022   28121  Gasoline   \n",
       "3  Audi                 noise        2016   61258  Gasoline   \n",
       "4  Audi  A6 2.0T Premium Plus        2018   59000  Gasoline   \n",
       "\n",
       "                                              engine        transmission  \\\n",
       "0       240.0HP 2.0L 4 Cylinder Engine Gasoline Fuel         6-Speed A/T   \n",
       "1  395.0HP 3.0L Straight 6 Cylinder Engine Gasoli...         8-Speed A/T   \n",
       "2                    3.5L V6 24V PDI DOHC Twin Turbo  10-Speed Automatic   \n",
       "3                                     2.0 Liter TFSI           Automatic   \n",
       "4       252.0HP 2.0L 4 Cylinder Engine Gasoline Fuel                 A/T   \n",
       "\n",
       "  ext_col int_col       accident clean_title  Vehicle_Age  Mileage_per_Year  \\\n",
       "0   White   Beige  None reported         Yes            9      10888.888889   \n",
       "1  Silver   Black  None reported         Yes            4       2285.500000   \n",
       "2   White   Ebony  None reported     missing            2      14060.500000   \n",
       "3   noise   Black  None reported     missing            8       7657.250000   \n",
       "4    Gray   Black  None reported         Yes            6       9833.333333   \n",
       "\n",
       "   milage_with_age  Mileage_per_Year_with_age  Is_Luxury_Brand      LGBM_MAE  \\\n",
       "0     81078.503981                9008.722665                1  16402.458527   \n",
       "1     34258.886442                8564.721611                1  57252.126782   \n",
       "2     17877.043403                8938.521702                0  49451.472824   \n",
       "3     75999.679762                9499.959970                1  25768.824542   \n",
       "4     52105.532436                8684.255406                1  27677.661661   \n",
       "\n",
       "   LGBM_MSE_diff  \n",
       "0    4280.396289  \n",
       "1   15875.930352  \n",
       "2    8181.763131  \n",
       "3    3535.516228  \n",
       "4    4005.789445  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [log_evaluation(period=300), early_stopping(stopping_rounds=200)]\n",
    "\n",
    "cat_cols = train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"cat_cols--------{cat_cols}\")\n",
    "\n",
    "\n",
    "def get_MAE_oof(df, target, lgb_params, cat_params=None, model_type='LGBM'):\n",
    "\n",
    "    \n",
    "    oof_predictions = np.zeros(len(df))\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    models = []\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
    "        print(f\"Training fold {fold + 1}/{5} with {model_type}\")\n",
    "\n",
    "        X_train, X_val = df.iloc[train_idx], df.iloc[val_idx]\n",
    "        y_train, y_val = target.iloc[train_idx], target.iloc[val_idx]\n",
    "\n",
    "        if model_type == 'LGBM':\n",
    "            train_data = lgb.Dataset(X_train, label=y_train)\n",
    "            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "            \n",
    "            model = lgb.train(\n",
    "                lgb_params,\n",
    "                train_data,\n",
    "                valid_sets=[train_data, val_data],\n",
    "                valid_names=['train', 'valid'],\n",
    "                callbacks=callbacks    \n",
    "            )\n",
    "        \n",
    "        elif model_type == 'CAT':\n",
    "            train_data = Pool(data=X_train, label=y_train , cat_features=cat_cols)\n",
    "            val_data = Pool(data=X_val, label=y_val , cat_features=cat_cols )\n",
    "            \n",
    "            model = CatBoostRegressor(**cat_params)\n",
    "            model.fit(train_data, eval_set=val_data, verbose=150, early_stopping_rounds=200)\n",
    "        \n",
    "        models.append(model)\n",
    "        \n",
    "        if model_type == 'LGBM':\n",
    "            pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "        elif model_type == 'CAT':\n",
    "            pred = model.predict(X_val)\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "        rmse_scores.append(rmse)\n",
    "\n",
    "        print(f'{model_type} Fold RMSE: {rmse}')\n",
    "        \n",
    "        oof_predictions[val_idx] = pred\n",
    "        \n",
    "    print(f'Mean RMSE: {np.mean(rmse_scores)}')\n",
    "    return oof_predictions, models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'MAE',\n",
    "    'n_estimators': 1000,\n",
    "    'random_state': 1,\n",
    "}\n",
    "\n",
    "oof_predictions_lgbm, models_lgbm = get_MAE_oof(X, y, lgb_params, model_type='LGBM')\n",
    "X['LGBM_MAE'] = oof_predictions_lgbm\n",
    "\n",
    "\n",
    "LGBM_preds = np.zeros(len(test))\n",
    "for model in models_lgbm:\n",
    "    LGBM_preds += model.predict(test) / len(models_lgbm)\n",
    "test['LGBM_MAE'] = LGBM_preds\n",
    "\n",
    "\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'MSE',\n",
    "    'n_estimators': 1000,\n",
    "    'random_state': 1,\n",
    "}\n",
    "\n",
    "oof_predictions_lgbm, models_lgbm = get_MAE_oof(X, y, lgb_params, model_type='LGBM')\n",
    "\n",
    "X['LGBM_MSE_diff'] = oof_predictions_lgbm - X['LGBM_MAE']\n",
    "\n",
    "\n",
    "LGBM_preds = np.zeros(len(test))\n",
    "for model in models_lgbm:\n",
    "    LGBM_preds += model.predict(test) / len(models_lgbm)\n",
    "test['LGBM_MSE_diff'] = LGBM_preds - test['LGBM_MAE']\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b56712b",
   "metadata": {
    "papermill": {
     "duration": 0.007141,
     "end_time": "2025-09-12T18:09:42.939177",
     "exception": false,
     "start_time": "2025-09-12T18:09:42.932036",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## AutoGluon TabularPredictor\n",
    "\n",
    "Fits an AutoGluon TabularPredictor on the engineered features, using RMSE as the evaluation metric and restricting models to GBM and CatBoost. Automatically uses GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd075dc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T18:09:42.954296Z",
     "iopub.status.busy": "2025-09-12T18:09:42.953959Z",
     "iopub.status.idle": "2025-09-12T19:09:44.859777Z",
     "shell.execute_reply": "2025-09-12T19:09:44.858897Z"
    },
    "papermill": {
     "duration": 3601.915094,
     "end_time": "2025-09-12T19:09:44.861231",
     "exception": false,
     "start_time": "2025-09-12T18:09:42.946137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250912_180942\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       29.96 GB / 31.35 GB (95.6%)\n",
      "Disk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
      "\t\tContext path: \"/kaggle/working/AutogluonModels/ag-20250912_180942/ds_sub_fit/sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n",
      "Beginning AutoGluon training ... Time limit = 899s\n",
      "AutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20250912_180942/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    171148\n",
      "Train Data Columns: 18\n",
      "Label Column:       price\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    30697.22 MB\n",
      "\tTrain Data (Original)  Memory Usage: 13.63 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 9 | ['brand', 'model', 'fuel_type', 'engine', 'transmission', ...]\n",
      "\t\t('float', [])    : 5 | ['Mileage_per_Year', 'milage_with_age', 'Mileage_per_Year_with_age', 'LGBM_MAE', 'LGBM_MSE_diff']\n",
      "\t\t('int', [])      : 4 | ['model_year', 'milage', 'Vehicle_Age', 'Is_Luxury_Brand']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 8 | ['brand', 'model', 'fuel_type', 'engine', 'transmission', ...]\n",
      "\t\t('float', [])     : 5 | ['Mileage_per_Year', 'milage_with_age', 'Mileage_per_Year_with_age', 'LGBM_MAE', 'LGBM_MSE_diff']\n",
      "\t\t('int', [])       : 3 | ['model_year', 'milage', 'Vehicle_Age']\n",
      "\t\t('int', ['bool']) : 2 | ['clean_title', 'Is_Luxury_Brand']\n",
      "\t0.5s = Fit runtime\n",
      "\t18 features in original data used to generate 18 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 12.41 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Included models: ['GBM', 'CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "Fitting 36 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 598.71s of the 898.29s of remaining time.\n",
      "Will use sequential fold fitting strategy because import of ray failed. Reason: ray==2.47.1 detected. 2.10.0 <= ray < 2.45.0 is required. You can use pip to install certain version of ray `pip install \"ray>=2.10.0,<2.45.0\"`\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-73004.404\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.58s\t = Training   runtime\n",
      "\t0.58s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 581.41s of the 880.99s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-73294.77\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.94s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 572.03s of the 871.61s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-72932.4866\t = Validation score   (-root_mean_squared_error)\n",
      "\t215.98s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 355.56s of the 655.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-73997.9299\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.82s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 340.00s of the 639.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-72942.0545\t = Validation score   (-root_mean_squared_error)\n",
      "\t137.2s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 202.35s of the 501.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-73179.7205\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.6s\t = Training   runtime\n",
      "\t1.87s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 172.47s of the 472.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 195.\n",
      "\t-72996.1429\t = Validation score   (-root_mean_squared_error)\n",
      "\t128.37s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 43.51s of the 343.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 735. Best iteration is:\n",
      "\t[676]\tvalid_set's rmse: 64351.3\n",
      "\t-72941.2536\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.2s\t = Training   runtime\n",
      "\t3.33s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 11.70s of the 311.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 13.\n",
      "\tRan out of time, early stopping on iteration 13.\n",
      "\tRan out of time, early stopping on iteration 15.\n",
      "\tRan out of time, early stopping on iteration 16.\n",
      "\tRan out of time, early stopping on iteration 17.\n",
      "\tRan out of time, early stopping on iteration 19.\n",
      "\tRan out of time, early stopping on iteration 22.\n",
      "\tRan out of time, early stopping on iteration 29.\n",
      "\t-74506.6739\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.9s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 0.56s of the 300.13s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\tTime limit exceeded... Skipping CatBoost_r13_BAG_L1.\n",
      "Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 0.17s of the 299.74s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\tWarning: Model has no time left to train, skipping model... (Time Left = -0.0s)\n",
      "\tTime limit exceeded... Skipping LightGBM_r188_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 299.34s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_r96_BAG_L1': 0.381, 'CatBoost_BAG_L1': 0.19, 'CatBoost_r9_BAG_L1': 0.19, 'CatBoost_r177_BAG_L1': 0.143, 'LightGBM_r131_BAG_L1': 0.095}\n",
      "\t-72855.2497\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Included models: ['GBM', 'CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "Fitting 36 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 299.03s of the 299.00s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-73009.4297\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.59s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 284.73s of the 284.70s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-73260.9015\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.21s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 270.91s of the 270.88s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 271.\n",
      "\t-72908.6262\t = Validation score   (-root_mean_squared_error)\n",
      "\t143.94s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 126.51s of the 126.48s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-73930.6367\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.21s\t = Training   runtime\n",
      "\t0.55s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 105.36s of the 105.33s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 141.\n",
      "\t-72916.4267\t = Validation score   (-root_mean_squared_error)\n",
      "\t81.98s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 22.97s of the 22.94s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 141. Best iteration is:\n",
      "\t[139]\tvalid_set's rmse: 66309\n",
      "\tRan out of time, early stopping on iteration 137. Best iteration is:\n",
      "\t[137]\tvalid_set's rmse: 77050.9\n",
      "\tRan out of time, early stopping on iteration 154. Best iteration is:\n",
      "\t[154]\tvalid_set's rmse: 61597.3\n",
      "\tRan out of time, early stopping on iteration 163. Best iteration is:\n",
      "\t[159]\tvalid_set's rmse: 73659.5\n",
      "\tRan out of time, early stopping on iteration 172. Best iteration is:\n",
      "\t[160]\tvalid_set's rmse: 85550.1\n",
      "\tRan out of time, early stopping on iteration 173. Best iteration is:\n",
      "\t[172]\tvalid_set's rmse: 86904.8\n",
      "\tRan out of time, early stopping on iteration 204. Best iteration is:\n",
      "\t[184]\tvalid_set's rmse: 64539.9\n",
      "\t-73341.218\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.0s\t = Training   runtime\n",
      "\t0.96s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 0.59s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_r96_BAG_L1': 0.36, 'CatBoost_r9_BAG_L1': 0.2, 'CatBoost_BAG_L1': 0.16, 'CatBoost_r177_BAG_L1': 0.12, 'LightGBM_r131_BAG_L1': 0.08, 'CatBoost_BAG_L2': 0.04, 'CatBoost_r177_BAG_L2': 0.04}\n",
      "\t-72855.2554\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.39s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 898.65s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3506.8 rows/s (21394 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/AutogluonModels/ag-20250912_180942/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                   model  score_holdout     score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0     CatBoost_r9_BAG_L1  -69319.263903 -72996.142888  root_mean_squared_error        0.385823       0.391819  128.372553                 0.385823                0.391819         128.372553            1       True          7\n",
      "1    WeightedEnsemble_L3  -69346.830499 -72855.255376  root_mean_squared_error        7.867902       7.834054  809.904564                 0.004710                0.003745           0.393752            3       True         17\n",
      "2    WeightedEnsemble_L2  -69351.069947 -72855.249661  root_mean_squared_error        6.107082       6.106123  537.626386                 0.003754                0.006150           0.279378            2       True         10\n",
      "3   CatBoost_r177_BAG_L2  -69364.758580 -72916.426661  root_mean_squared_error        7.697675       7.653552  665.566020                 0.124953                0.136986          81.975220            2       True         15\n",
      "4        CatBoost_BAG_L2  -69372.724627 -72908.626237  root_mean_squared_error        7.738239       7.693324  727.535591                 0.165517                0.176758         143.944792            2       True         13\n",
      "5        LightGBM_BAG_L2  -69387.673558 -73260.901507  root_mean_squared_error        7.908472       7.834857  596.801866                 0.335750                0.318290          13.211066            2       True         12\n",
      "6      LightGBMXT_BAG_L2  -69440.201466 -73009.429684  root_mean_squared_error        8.016733       7.935166  597.178493                 0.444011                0.418600          13.587693            2       True         11\n",
      "7    LightGBM_r96_BAG_L1  -69458.983386 -72941.253635  root_mean_squared_error        3.336078       3.332754   28.199094                 3.336078                3.332754          28.199094            1       True          8\n",
      "8   LightGBM_r131_BAG_L2  -69460.409566 -73341.218003  root_mean_squared_error        8.605119       8.474958  604.588384                 1.032397                0.958391          20.997584            2       True         16\n",
      "9   LightGBM_r131_BAG_L1  -69462.588719 -73179.720521  root_mean_squared_error        1.905745       1.874302   27.597621                 1.905745                1.874302          27.597621            1       True          6\n",
      "10       CatBoost_BAG_L1  -69469.753511 -72932.486565  root_mean_squared_error        0.259913       0.270796  215.976732                 0.259913                0.270796         215.976732            1       True          3\n",
      "11  CatBoost_r177_BAG_L1  -69515.121978 -72942.054510  root_mean_squared_error        0.215770       0.230303  137.201008                 0.215770                0.230303         137.201008            1       True          5\n",
      "12     LightGBMXT_BAG_L1  -69543.844428 -73004.403998  root_mean_squared_error        0.616768       0.582045   11.583609                 0.616768                0.582045          11.583609            1       True          1\n",
      "13       LightGBM_BAG_L1  -69582.200805 -73294.770041  root_mean_squared_error        0.261185       0.259417    8.940287                 0.261185                0.259417           8.940287            1       True          2\n",
      "14  LightGBMLarge_BAG_L2  -69698.558915 -73930.636681  root_mean_squared_error        8.167535       8.068348  603.799775                 0.594813                0.551782          20.208975            2       True         14\n",
      "15  LightGBMLarge_BAG_L1  -69809.631370 -73997.929913  root_mean_squared_error        0.530238       0.496303   14.821819                 0.530238                0.496303          14.821819            1       True          4\n",
      "16  CatBoost_r137_BAG_L1  -70822.846948 -74506.673898  root_mean_squared_error        0.061203       0.078828   10.898077                 0.061203                0.078828          10.898077            1       True          9\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t912s\t = DyStack   runtime |\t2688s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 2688s\n",
      "AutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20250912_180942\"\n",
      "Train Data Rows:    192542\n",
      "Train Data Columns: 18\n",
      "Label Column:       price\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    30453.26 MB\n",
      "\tTrain Data (Original)  Memory Usage: 15.33 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 9 | ['brand', 'model', 'fuel_type', 'engine', 'transmission', ...]\n",
      "\t\t('float', [])    : 5 | ['Mileage_per_Year', 'milage_with_age', 'Mileage_per_Year_with_age', 'LGBM_MAE', 'LGBM_MSE_diff']\n",
      "\t\t('int', [])      : 4 | ['model_year', 'milage', 'Vehicle_Age', 'Is_Luxury_Brand']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 8 | ['brand', 'model', 'fuel_type', 'engine', 'transmission', ...]\n",
      "\t\t('float', [])     : 5 | ['Mileage_per_Year', 'milage_with_age', 'Mileage_per_Year_with_age', 'LGBM_MAE', 'LGBM_MSE_diff']\n",
      "\t\t('int', [])       : 3 | ['model_year', 'milage', 'Vehicle_Age']\n",
      "\t\t('int', ['bool']) : 2 | ['clean_title', 'Is_Luxury_Brand']\n",
      "\t0.4s = Fit runtime\n",
      "\t18 features in original data used to generate 18 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 13.96 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.49s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Included models: ['GBM', 'CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "Fitting 36 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1791.48s of the 2687.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-72630.1328\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.33s\t = Training   runtime\n",
      "\t0.63s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1778.32s of the 2674.73s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-72802.5232\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.26s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 1766.44s of the 2662.85s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-72581.2268\t = Validation score   (-root_mean_squared_error)\n",
      "\t208.61s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1557.34s of the 2453.75s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-73339.5496\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.82s\t = Training   runtime\n",
      "\t0.62s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 1539.62s of the 2436.03s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-72575.1788\t = Validation score   (-root_mean_squared_error)\n",
      "\t134.75s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 1404.44s of the 2300.85s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-72742.4959\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.33s\t = Training   runtime\n",
      "\t2.48s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 1369.18s of the 2265.59s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-72604.81\t = Validation score   (-root_mean_squared_error)\n",
      "\t143.95s\t = Training   runtime\n",
      "\t0.43s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 1224.60s of the 2121.01s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 71332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-72593.096\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.19s\t = Training   runtime\n",
      "\t3.74s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 1189.39s of the 2085.80s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-72583.8924\t = Validation score   (-root_mean_squared_error)\n",
      "\t183.05s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 1005.91s of the 1902.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 735.\n",
      "\tRan out of time, early stopping on iteration 810.\n",
      "\t-72554.9866\t = Validation score   (-root_mean_squared_error)\n",
      "\t717.12s\t = Training   runtime\n",
      "\t0.81s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 287.58s of the 1183.99s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-72690.3916\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.49s\t = Training   runtime\n",
      "\t1.36s\t = Validation runtime\n",
      "Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 265.41s of the 1161.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-72680.9296\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.9s\t = Training   runtime\n",
      "\t0.85s\t = Validation runtime\n",
      "Fitting model: CatBoost_r50_BAG_L1 ... Training model for up to 249.42s of the 1145.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-72577.552\t = Validation score   (-root_mean_squared_error)\n",
      "\t54.98s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: CatBoost_r69_BAG_L1 ... Training model for up to 194.00s of the 1090.41s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 188.\n",
      "\tRan out of time, early stopping on iteration 212.\n",
      "\t-72576.2583\t = Validation score   (-root_mean_squared_error)\n",
      "\t164.52s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: LightGBM_r161_BAG_L1 ... Training model for up to 29.02s of the 925.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 113. Best iteration is:\n",
      "\t[113]\tvalid_set's rmse: 70710.1\n",
      "\tRan out of time, early stopping on iteration 113. Best iteration is:\n",
      "\t[113]\tvalid_set's rmse: 63480.5\n",
      "\tRan out of time, early stopping on iteration 118. Best iteration is:\n",
      "\t[118]\tvalid_set's rmse: 94506.2\n",
      "\tRan out of time, early stopping on iteration 120. Best iteration is:\n",
      "\t[120]\tvalid_set's rmse: 72208.7\n",
      "\tRan out of time, early stopping on iteration 120. Best iteration is:\n",
      "\t[120]\tvalid_set's rmse: 73339.3\n",
      "\tRan out of time, early stopping on iteration 131. Best iteration is:\n",
      "\t[131]\tvalid_set's rmse: 66453\n",
      "\tRan out of time, early stopping on iteration 138. Best iteration is:\n",
      "\t[138]\tvalid_set's rmse: 69545.4\n",
      "\tRan out of time, early stopping on iteration 145. Best iteration is:\n",
      "\t[145]\tvalid_set's rmse: 70819\n",
      "\t-73162.3221\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.98s\t = Training   runtime\n",
      "\t1.9s\t = Validation runtime\n",
      "Fitting model: CatBoost_r70_BAG_L1 ... Training model for up to 0.42s of the 896.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\tWarning: Model has no time left to train, skipping model... (Time Left = -0.0s)\n",
      "\tTime limit exceeded... Skipping CatBoost_r70_BAG_L1.\n",
      "Fitting model: LightGBM_r196_BAG_L1 ... Training model for up to 0.03s of the 896.44s of remaining time.\n",
      "\tWarning: Model has no time left to train, skipping model... (Time Left = -0.0s)\n",
      "\tTime limit exceeded... Skipping LightGBM_r196_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 896.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_r96_BAG_L1': 0.25, 'CatBoost_r9_BAG_L1': 0.208, 'LightGBM_r130_BAG_L1': 0.208, 'CatBoost_r13_BAG_L1': 0.125, 'CatBoost_r177_BAG_L1': 0.083, 'CatBoost_r69_BAG_L1': 0.083, 'CatBoost_r50_BAG_L1': 0.042}\n",
      "\t-72485.0245\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Included models: ['GBM', 'CAT'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "Fitting 36 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 895.66s of the 895.61s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-72623.5278\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.35s\t = Training   runtime\n",
      "\t0.54s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 876.32s of the 876.27s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-72873.6235\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.74s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 858.80s of the 858.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-72500.3804\t = Validation score   (-root_mean_squared_error)\n",
      "\t154.28s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 703.93s of the 703.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-73524.0065\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.34s\t = Training   runtime\n",
      "\t0.49s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 680.64s of the 680.59s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-72488.3925\t = Validation score   (-root_mean_squared_error)\n",
      "\t100.65s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 579.43s of the 579.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-72820.1757\t = Validation score   (-root_mean_squared_error)\n",
      "\t37.25s\t = Training   runtime\n",
      "\t1.67s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L2 ... Training model for up to 539.90s of the 539.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-72549.6981\t = Validation score   (-root_mean_squared_error)\n",
      "\t157.82s\t = Training   runtime\n",
      "\t0.41s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L2 ... Training model for up to 381.25s of the 381.20s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-72566.0299\t = Validation score   (-root_mean_squared_error)\n",
      "\t30.37s\t = Training   runtime\n",
      "\t1.69s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L2 ... Training model for up to 348.73s of the 348.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\t-72491.4979\t = Validation score   (-root_mean_squared_error)\n",
      "\t95.05s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: CatBoost_r13_BAG_L2 ... Training model for up to 253.12s of the 253.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 152.\n",
      "\tRan out of time, early stopping on iteration 158.\n",
      "\tRan out of time, early stopping on iteration 163.\n",
      "\tRan out of time, early stopping on iteration 168.\n",
      "\tRan out of time, early stopping on iteration 179.\n",
      "\tRan out of time, early stopping on iteration 191.\n",
      "\tRan out of time, early stopping on iteration 212.\n",
      "\tRan out of time, early stopping on iteration 257.\n",
      "\t-72532.3306\t = Validation score   (-root_mean_squared_error)\n",
      "\t242.02s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L2 ... Training model for up to 10.48s of the 10.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=2, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\tvalid_set's rmse: 64498.7\n",
      "\tRan out of time, early stopping on iteration 2. Best iteration is:\n",
      "\t[2]\tvalid_set's rmse: 80484.5\n",
      "\tRan out of time, early stopping on iteration 4. Best iteration is:\n",
      "\t[4]\tvalid_set's rmse: 76803.1\n",
      "\tRan out of time, early stopping on iteration 6. Best iteration is:\n",
      "\t[6]\tvalid_set's rmse: 91719.8\n",
      "\tRan out of time, early stopping on iteration 8. Best iteration is:\n",
      "\t[8]\tvalid_set's rmse: 71432.2\n",
      "\tRan out of time, early stopping on iteration 11. Best iteration is:\n",
      "\t[11]\tvalid_set's rmse: 69208.8\n",
      "\tRan out of time, early stopping on iteration 16. Best iteration is:\n",
      "\t[16]\tvalid_set's rmse: 76958.9\n",
      "\tRan out of time, early stopping on iteration 26. Best iteration is:\n",
      "\t[26]\tvalid_set's rmse: 76854.4\n",
      "\t-76379.6075\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.65s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 0.16s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_r177_BAG_L2': 0.36, 'LightGBM_r96_BAG_L1': 0.12, 'LightGBM_r130_BAG_L1': 0.12, 'CatBoost_r9_BAG_L2': 0.12, 'CatBoost_r137_BAG_L2': 0.12, 'CatBoost_r9_BAG_L1': 0.08, 'CatBoost_r13_BAG_L1': 0.04, 'LightGBMXT_BAG_L2': 0.04}\n",
      "\t-72461.6427\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.71s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2689.0s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1537.3 rows/s (24068 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/AutogluonModels/ag-20250912_180942\")\n"
     ]
    }
   ],
   "source": [
    "X['price'] = y\n",
    "\n",
    "predictor = TabularPredictor(label='price',\n",
    "                            eval_metric='rmse',\n",
    "                            problem_type='regression').fit(X,\n",
    "                                                       presets='best_quality',\n",
    "                                                       time_limit=3600*1,\n",
    "                                                       verbosity=2,\n",
    "                                                       num_gpus=0,\n",
    "                                                       included_model_types=['GBM', 'CAT']\n",
    "                                                      )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b997e6",
   "metadata": {
    "papermill": {
     "duration": 0.024595,
     "end_time": "2025-09-12T19:09:44.910580",
     "exception": false,
     "start_time": "2025-09-12T19:09:44.885985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prediction & Submission Blending\n",
    "\n",
    "Generates predictions on the test set, blends them with an existing Kaggle solution, and creates the final submission file for the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40cd5b25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T19:09:44.960862Z",
     "iopub.status.busy": "2025-09-12T19:09:44.960571Z",
     "iopub.status.idle": "2025-09-12T19:11:04.004410Z",
     "shell.execute_reply": "2025-09-12T19:11:04.003654Z"
    },
    "papermill": {
     "duration": 79.095845,
     "end_time": "2025-09-12T19:11:04.030728",
     "exception": false,
     "start_time": "2025-09-12T19:09:44.934883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>188533</td>\n",
       "      <td>18975.962045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188534</td>\n",
       "      <td>78099.202956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>188535</td>\n",
       "      <td>57735.001267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>188536</td>\n",
       "      <td>29839.036537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>188537</td>\n",
       "      <td>31191.854321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id         price\n",
       "0  188533  18975.962045\n",
       "1  188534  78099.202956\n",
       "2  188535  57735.001267\n",
       "3  188536  29839.036537\n",
       "4  188537  31191.854321"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predictor.predict(test)\n",
    "\n",
    "\n",
    "sub_blend = pd.read_csv('/kaggle/input/top-5-blended-car-prices/submission_9.csv')\n",
    "sample_sub = pd.read_csv('/kaggle/input/hackathon-qualification/archive/sample_submission.csv')\n",
    "sample_sub['price'] =  y_pred * 0.55 + sub_blend['price'] * 0.45\n",
    "sample_sub.to_csv('/kaggle/working/./submission.csv', index=False)\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2791c59a",
   "metadata": {
    "papermill": {
     "duration": 0.024431,
     "end_time": "2025-09-12T19:11:04.079664",
     "exception": false,
     "start_time": "2025-09-12T19:11:04.055233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13542141,
     "isSourceIdPinned": false,
     "sourceId": 113485,
     "sourceType": "competition"
    },
    {
     "datasetId": 3742543,
     "sourceId": 6478229,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 196651220,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3777.307658,
   "end_time": "2025-09-12T19:11:07.600268",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-12T18:08:10.292610",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
